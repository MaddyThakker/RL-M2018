{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = np.array(['^', '<', 'd', '>'])\n",
    "\n",
    "def get_next_state(state, action):\n",
    "    reward = -1\n",
    "    if(state[0] == 0 and state[1] == 0):\n",
    "        return 0, state\n",
    "    if(state[0] == 3 and state[1] == 3):\n",
    "        return 0, state\n",
    "    if(action == 0):\n",
    "        next_state = [state[0] - 1, state[1]]\n",
    "    if(action == 1):\n",
    "        next_state = [state[0], state[1]-1]\n",
    "    if(action == 2):\n",
    "        next_state = [state[0] + 1, state[1]]\n",
    "    if(action == 3):\n",
    "        next_state = [state[0], state[1]+1]\n",
    "    if(next_state[0] < 0 or next_state[0] >= 4 or next_state[1] < 0 or next_state[1] >= 4):\n",
    "        next_state = state\n",
    "    return reward, next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Evaluation at Work\n",
      "[[ 0. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1.  0.]]\n",
      "Policy Evaluation at Work\n",
      "[[ 0.   -1.75 -2.   -2.  ]\n",
      " [-1.75 -2.   -2.   -2.  ]\n",
      " [-2.   -2.   -2.   -1.75]\n",
      " [-2.   -2.   -1.75  0.  ]]\n",
      "Policy Evaluation at Work\n",
      "[[ 0.     -2.4375 -2.9375 -3.    ]\n",
      " [-2.4375 -2.875  -3.     -2.9375]\n",
      " [-2.9375 -3.     -2.875  -2.4375]\n",
      " [-3.     -2.9375 -2.4375  0.    ]]\n",
      "Policy Evaluation at Work\n",
      "[[ 0.      -3.0625  -3.84375 -3.96875]\n",
      " [-3.0625  -3.71875 -3.90625 -3.84375]\n",
      " [-3.84375 -3.90625 -3.71875 -3.0625 ]\n",
      " [-3.96875 -3.84375 -3.0625   0.     ]]\n",
      "Policy Evaluation at Work\n",
      "[[ 0.        -3.65625   -4.6953125 -4.90625  ]\n",
      " [-3.65625   -4.484375  -4.78125   -4.6953125]\n",
      " [-4.6953125 -4.78125   -4.484375  -3.65625  ]\n",
      " [-4.90625   -4.6953125 -3.65625    0.       ]]\n",
      "breaking 9.972644932076946e-11 1e-10\n",
      "[[  0. -14. -20. -22.]\n",
      " [-14. -18. -20. -20.]\n",
      " [-20. -20. -18. -14.]\n",
      " [-22. -20. -14.   0.]]\n",
      "['^' '<' 'd' '>'] ['^' '<' 'd' '>'] ['^' '<' 'd' '>'] ['^' '<' 'd' '>'] \n",
      "['^' '<' 'd' '>'] ['^' '<' 'd' '>'] ['^' '<' 'd' '>'] ['^' '<' 'd' '>'] \n",
      "['^' '<' 'd' '>'] ['^' '<' 'd' '>'] ['^' '<' 'd' '>'] ['^' '<' 'd' '>'] \n",
      "['^' '<' 'd' '>'] ['^' '<' 'd' '>'] ['^' '<' 'd' '>'] ['^' '<' 'd' '>'] \n",
      "Policy Evaluation at Work\n",
      "[[  0.  -1. -15. -21.]\n",
      " [ -1. -15. -19. -15.]\n",
      " [-15. -19. -15.  -1.]\n",
      " [-21. -15.  -1.   0.]]\n",
      "Policy Evaluation at Work\n",
      "[[  0.  -1.  -2. -16.]\n",
      " [ -1.  -2. -16.  -2.]\n",
      " [ -2. -16.  -2.  -1.]\n",
      " [-16.  -2.  -1.   0.]]\n",
      "Policy Evaluation at Work\n",
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n",
      "Policy Evaluation at Work\n",
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n",
      "breaking 0.0 1e-10\n",
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n",
      "['^'] ['<'] ['<'] ['<'] \n",
      "['^'] ['<'] ['<'] ['d'] \n",
      "['^'] ['^'] ['d'] ['d'] \n",
      "['^'] ['>'] ['>'] ['^'] \n",
      "Policy Evaluation at Work\n",
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n",
      "breaking 0.0 1e-10\n",
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n",
      "['^'] ['<'] ['<'] ['<'] \n",
      "['^'] ['^'] ['^'] ['d'] \n",
      "['^'] ['^'] ['d'] ['d'] \n",
      "['^'] ['>'] ['>'] ['^'] \n",
      "policy stable, breaking\n"
     ]
    }
   ],
   "source": [
    "theta = 1e-10\n",
    "gamma = 1 # undiscounted task\n",
    "value = np.zeros([4, 4])\n",
    "state = [[i, j] for i in range(4) for j in range(4)]\n",
    "all_actions = [ [ [0, 1, 2, 3] for i in range(4) ] for j in range(4) ]\n",
    "policies = np.zeros(value.shape)\n",
    "while True:\n",
    "    print_iter = 5\n",
    "\n",
    "    while True:\n",
    "        delta = 0\n",
    "        \n",
    "        new_value = np.zeros([4, 4])\n",
    "        \n",
    "        \n",
    "        for s in state:            \n",
    "            v_iter = []\n",
    "            for action in all_actions[s[0]][s[1]]:\n",
    "                reward, next_state = get_next_state(s, action)\n",
    "                new_value[s[0]][s[1]] += (1/len(all_actions[s[0]][s[1]]))*(reward + gamma*value[next_state[0]][next_state[1]])\n",
    "        \n",
    "        delta = np.sum(abs(value - new_value))\n",
    "        \n",
    "        value = new_value\n",
    "        if(print_iter > 0):\n",
    "            print_iter-=1\n",
    "            print(\"Policy Evaluation at Work\")\n",
    "            print(value)\n",
    "        if(delta < theta):\n",
    "            print(\"breaking\", delta, theta)\n",
    "            break\n",
    "                \n",
    "    print(value)      \n",
    "    for action_lis in all_actions:\n",
    "        for specific_action in action_lis:\n",
    "            print(chars[specific_action], end = ' ')\n",
    "        print()\n",
    "        \n",
    "        \n",
    "    policy_stable = True\n",
    "    \n",
    "    for s in state:\n",
    "        \n",
    "        old_action = policies[s[0]][s[1]]\n",
    "        v_iter = []\n",
    "        \n",
    "        for action in [0, 1, 2, 3]:\n",
    "            reward, next_state = get_next_state(s, action)\n",
    "            v_iter.append((reward + gamma*value[next_state[0]][next_state[1]]))\n",
    "        \n",
    "        v = max(v_iter)\n",
    "        \n",
    "        v_iter = np.array(v_iter)\n",
    "        best_action = np.argmax(v_iter)\n",
    "\n",
    "        policies[s[0]][s[1]]  = best_action\n",
    "        all_actions[s[0]][s[1]] = [best_action]\n",
    "        \n",
    "        if policy_stable and best_action != old_action:\n",
    "            policy_stable = False\n",
    "    \n",
    "    if(policy_stable):\n",
    "        print(\"policy stable, breaking\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^  <  <  <  \n",
      "^  ^  ^  d  \n",
      "^  ^  d  d  \n",
      "^  >  >  ^  \n"
     ]
    }
   ],
   "source": [
    "for x in policies:\n",
    "    for y in x:\n",
    "        print(chars[int(y)], end = '  ')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### fix to the bug mentioned in Exercise 4.4\n",
    "\n",
    "#### Q -The algorithm may never terminate if the policy continually switches between two or more policies that are equally good\n",
    "\n",
    "#### Ans - Suppose that there are 2 policies that are equally good, the difference between the 2 policies is just for a single state, 2 different actions can be selected (both being equally good). The algorithm takes the argmax which inturn can select any of the equally likely actions for any state. If they form a cycle such that different actions are selected in every iteration; the algorithm may never terminate. \n",
    "#### The fix is just to take the maximum action for each state such that it occurs at the smallest index in the list of actions to prevent different actions being selected. np.argmax() does exaclty this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Iteration - \n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "New Iteration - \n",
      "[[ 0. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1.  0.]]\n",
      "New Iteration - \n",
      "[[ 0. -1. -2. -2.]\n",
      " [-1. -2. -2. -2.]\n",
      " [-2. -2. -2. -1.]\n",
      " [-2. -2. -1.  0.]]\n",
      "New Iteration - \n",
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n",
      "breaking 0 1e-10\n"
     ]
    }
   ],
   "source": [
    "theta = 1e-10\n",
    "gamma = 1 # undiscounted task\n",
    "value = np.zeros([4, 4])\n",
    "all_actions = [ [ [0, 1, 2, 3] for i in range(4) ] for j in range(4) ]\n",
    "while True:\n",
    "    print(\"New Iteration - \")\n",
    "    print(value)\n",
    "    delta = 0\n",
    "    for s in state:\n",
    "        v = value[s[0]][s[1]]\n",
    "        v_iter = []\n",
    "        for action in all_actions[s[0]][s[1]]:\n",
    "            reward, next_state = get_next_state(s, action)\n",
    "            v_iter.append(reward + gamma*value[next_state[0]][next_state[1]])\n",
    "        value[s[0]][s[1]] = np.max(v_iter)\n",
    "        delta = max(delta, abs(v - value[s[0]][s[1]]))\n",
    "    if(delta < theta):\n",
    "        print(\"breaking\", delta, theta)\n",
    "        break\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = np.chararray([4, 4],unicode=True)\n",
    "for s in state:\n",
    "    v_iter = []\n",
    "    for action in all_actions[s[0]][s[1]]:\n",
    "        reward, next_state = get_next_state(s, action)\n",
    "        v_iter.append(reward + gamma*value[next_state[0]][next_state[1]])\n",
    "    policy[s[0]][s[1]] = chars[(np.argmax(v_iter))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['^' '<' '<' '<']\n",
      " ['^' '^' '^' 'd']\n",
      " ['^' '^' 'd' 'd']\n",
      " ['^' '>' '>' '^']]\n"
     ]
    }
   ],
   "source": [
    "print(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As we can see, with every iteration Value Function is improving; also optimal policy is printed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
